{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "punctuation = '''!;:',\"-.?'''"
      ],
      "metadata": {
        "id": "PTAQ-q_0c7hb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(input_string):\n",
        "  string = ''\n",
        "  for i in input_string:\n",
        "    if i in punctuation:\n",
        "      string += ' '\n",
        "    else:\n",
        "      string += i\n",
        "\n",
        "  return string"
      ],
      "metadata": {
        "id": "63QU0omWdEnk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace(input_string,ele,ele2):\n",
        "  string = ''\n",
        "  for i in input_string:\n",
        "    if i == ele:\n",
        "      string += ele2\n",
        "    else:\n",
        "      string += i\n",
        "  return string"
      ],
      "metadata": {
        "id": "OWU7egP9gmhI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count(input_string):\n",
        "  p_count =0\n",
        "  s_count =0\n",
        "  c_count =0\n",
        "  for i in input_string:\n",
        "    if i in punctuation:\n",
        "      p_count += 1\n",
        "    if i == ' ':\n",
        "      s_count +=1\n",
        "    c_count += 1\n",
        "  return p_count,s_count,c_count"
      ],
      "metadata": {
        "id": "zqLkZ0BihWjR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsLRE66udFyI",
        "outputId": "cc3ac669-9183-4d98-95ac-37379c33f6aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence  Article Talk Read View source View history Page semi-protected From Wikipedia, the free encyclopedia \"AI\" redirects here. For other uses, see AI (disambiguation), Artificial intelligence (disambiguation), and Intelligent agent. Part of a series on Artificial intelligence Anatomy-1751201 1280.png Major goals Approaches Philosophy History Technology Glossary vte Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.  AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[1]  As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.[2] For instance, optical character recognition is frequently excluded from things considered to be AI,[3] having become a routine technology.[4]  Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[5][6] followed by disappointment and the loss of funding (known as an \"AI winter\"),[7][8] followed by new approaches, success and renewed funding.[6][9] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[9][10]  The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.[a] General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.[11] To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques – including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.  The field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\".[b] This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[13] Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.[c]  History Main articles: History of artificial intelligence and Timeline of artificial intelligence  Silver didrachma from Crete depicting Talos, an ancient mythical automaton with artificial intelligence Artificial beings with intelligence appeared as storytelling devices in antiquity,[14] and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.[15] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[16]  The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.[17] This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.[18] The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".[19]  By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the \"heuristic search\" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.  The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[20] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[21]  The field of AI research was born at a workshop at Dartmouth College in 1956.[d][24] The attendees became the founders and leaders of AI research.[e] They and their students produced programs that the press described as \"astonishing\":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[g][26]  By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[27] and laboratories had been established around the world.[28]  Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.[29] Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[30] Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[31]  They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[32] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.[7]  In the early 1980s, AI research was revived by the commercial success of expert systems,[33] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[6] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[8]  Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems.[34] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[h]  Interest in neural networks and \"connectionism\" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[39] Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.  AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[40] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".[10]  Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[41] According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects.[i] He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[9]  In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\".[42] The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.[43]  Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[11]  Goals The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[a]  Reasoning, problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[44] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[45]  Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.[46] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[47]  Knowledge representation Main articles: Knowledge representation, Commonsense knowledge, Description logic, and Ontology  An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. Knowledge representation and knowledge engineering[48] allow AI programs to answer questions intelligently and make deductions about real-world facts.  A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[49] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[50]  AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[50] situations, events, states and time;[51] causes and effects;[52] knowledge about knowledge (what we know about what other people know);.[53] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[54] as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[55] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[47]  Formal knowledge representations are used in content-based indexing and retrieval,[56] scene interpretation,[57] clinical decision support,[58] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[59] and other areas.[60]  Learning Main article: Machine learning Machine learning (ML), a fundamental concept of AI research since the field's inception,[j] is the study of computer algorithms that improve automatically through experience.[k]  Unsupervised learning finds patterns in a stream of input.  Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\".[64]  In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[65]  Transfer learning is when the knowledge gained from one problem is applied to a new problem.[66]  Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[67]  Natural language processing Main article: Natural language processing  A parse tree represents the syntactic structure of a sentence according to some formal grammar. Natural language processing (NLP)[68] allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[69]  Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[46] and the breadth of commonsense knowledge.[55] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), \"Keyword spotting\" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[70] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[71]  Perception Main articles: Machine perception, Computer vision, and Speech recognition  Feature detection (pictured: edge detection) helps AI compose informative abstract structures out of raw data. Machine perception[72] is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[73] facial recognition, and object recognition.[74] Computer vision is the ability to analyze visual input.[75]  Social intelligence Main article: Affective computing  Kismet, a robot with rudimentary social skills[76] Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.[77] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[78] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[79]  General intelligence Main article: Artificial general intelligence A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[80] Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, \"master algorithm\" that could lead to AGI.[81] Others believe that anthropomorphic features like an artificial brain[82] or simulated child development[l] will someday reach a critical point where general intelligence emerges.  Tools Search and optimization Main articles: Search algorithm, Mathematical optimization, and Evolutionary computation AI can solve many problems by intelligently searching through many possible solutions.[83] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[84] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[85] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[86]  Simple exhaustive searches[87] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies.[88] Heuristics limit the search for solutions into a smaller sample size.[89]   A particle swarm seeking the global minimum A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[90] Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[91] Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[92]  Logic Main articles: Logic programming and Automated reasoning Logic[93] is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[94] and inductive logic programming is a method for learning.[95]  Several different forms of logic are used in AI research. Propositional logic[96] involves truth functions such as \"or\" and \"not\". First-order logic[97] adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[98] Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[54] Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[50] situation calculus, event calculus and fluent calculus (for representing events and time);[51] causal calculus;[52] belief calculus (belief revision); and modal logics.[53] Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[99]  Probabilistic methods for uncertain reasoning Main articles: Bayesian network, Hidden Markov model, Kalman filter, Particle filter, Decision theory, and Utility theory  Expectation-maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption. Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[100] Bayesian networks[101] are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[m][103] learning (using the expectation-maximization algorithm),[n][105] planning (using decision networks)[106] and perception (using dynamic Bayesian networks).[107] Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[107]  A key concept from the science of economics is \"utility\", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[108] and information value theory.[109] These tools include models such as Markov decision processes,[110] dynamic decision networks,[107] game theory and mechanism design.[111]  Classifiers and statistical learning methods Main articles: Statistical classification and Machine learning The simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if diamond then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[112]  A classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is the simplest and most widely used symbolic machine learning algorithm.[113] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[114] Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[115] The naive Bayes classifier is reportedly the \"most widely used learner\"[116] at Google, due in part to its scalability.[117] Neural networks are also used for classification.[118]  Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.[119]  Artificial neural networks Main articles: Artificial neural network and Connectionism  A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain. Neural networks[118] were inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.  Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.[120] Other learning techniques for neural networks are Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.[121]  The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[122]  Deep learning Representing Images on Multiple Layers of Abstraction in Deep Learning Representing images on multiple layers of abstraction in deep learning[123] Deep learning[124] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[125] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[126] and others.  Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[127] and creates a hierarchy similar to the organization of the animal visual cortex.[128]  In a recurrent neural network (RNN) the signal will propagate through a layer more than once;[129] thus, an RNN is an example of deep learning.[130] RNNs can be trained by gradient descent,[131] however long-term gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), known as the vanishing gradient problem.[132] The long short term memory (LSTM) technique can prevent this in most cases.[133]  Specialized languages and hardware Main articles: Programming languages for artificial intelligence and Hardware for artificial intelligence Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.  Applications Main article: Applications of artificial intelligence See also: Embodied cognition and Legal informatics  For this project of the artist Joseph Ayerle the AI had to learn the typical patterns in the colors and brushstrokes of Renaissance painter Raphael. The portrait shows the face of the actress Ornella Muti, \"painted\" by AI in the style of Raphael. AI is relevant to any intellectual task.[134] Modern artificial intelligence techniques are pervasive and are too numerous to list here.[135] Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[136]  In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search), targeting online advertisements,[137] recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic,[138][139] targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa),[140] autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok) , spam filtering and chatbots (such as Chat GPT).  There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage,[141] deepfakes,[142] medical diagnosis, military logistics, or supply chain management.  Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[143] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[144] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[145] Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[o] and Cepheus.[147] DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own.[148]  By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[149] DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[150] Other applications predict the result of judicial decisions,[151] create art (such as poetry or painting) and prove mathematical theorems.  Smart traffic lights   Artificially intelligent traffic lights use cameras with radar, ultrasonic acoustic location sensors, and predictive algorithms to improve traffic flow Smart traffic lights have been developed at Carnegie Mellon since 2009. Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities. It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[152]  Intellectual Property  AI Patent families for functional application categories and sub categories. Computer vision represents 49 percent of patent families related to a functional application in 2016. In 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G).[153] Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four.[154] The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.[154]  Philosophy Main article: Philosophy of artificial intelligence Defining artificial intelligence Main articles: Turing test, Intelligent agent, Dartmouth workshop, and Synthetic intelligence Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[155] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[155] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[156] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people[p] but \"it is usual to have a polite convention that everyone thinks\"[157]  Russell and Norvig agree with Turing that AI must be defined in terms of \"acting\" and not \"thinking\".[158] However, they are critical that the test compares machines to people. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[159] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[160]  McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world.\"[161] Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\".[162] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.  A definition that has also been adopted by Google[163][better source needed] - major practitionary in the field of AI. This definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.  Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history.[q] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.  Symbolic AI and its limits Main articles: Symbolic AI, Physical symbol systems hypothesis, Moravec's paradox, and Hubert Dreyfus's views on artificial intelligence Symbolic AI (or \"GOFAI\")[165] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[166]  However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[167] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[168] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[r][47]  The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[170][171] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.  Neat vs. scruffy Main article: Neats and scruffies \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning). This issue was actively discussed in the 70s and 80s,[172] but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed \"the victory of the neats\".[173]  Soft vs. hard computing Main article: Soft computing Finding a provably correct or optimal solution is intractable for many important problems.[46] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.  Narrow vs. general AI Main article: Artificial general intelligence AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[174][175] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.  Machine consciousness, sentience and mind Main articles: Philosophy of artificial intelligence and Artificial consciousness The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers \"don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.\"[176] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.  Consciousness Main articles: Hard problem of consciousness and Theory of mind David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[177] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = remove_punc(n)\n",
        "print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGUKVUGGdeaW",
        "outputId": "e366150e-1bac-425f-fb2b-e93bc6c02034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence  Article Talk Read View source View history Page semi protected From Wikipedia  the free encyclopedia  AI  redirects here  For other uses  see AI (disambiguation)  Artificial intelligence (disambiguation)  and Intelligent agent  Part of a series on Artificial intelligence Anatomy 1751201 1280 png Major goals Approaches Philosophy History Technology Glossary vte Artificial intelligence (AI) is intelligence—perceiving  synthesizing  and inferring information—demonstrated by machines  as opposed to intelligence displayed by non human animals and humans  Example tasks in which this is done include speech recognition  computer vision  translation between (natural) languages  as well as other mappings of inputs   AI applications include advanced web search engines (e g   Google Search)  recommendation systems (used by YouTube  Amazon and Netflix)  understanding human speech (such as Siri and Alexa)  self driving cars (e g   Waymo)  generative or creative tools (ChatGPT and AI art)  automated decision making and competing at the highest level in strategic game systems (such as chess and Go) [1]  As machines become increasingly capable  tasks considered to require  intelligence  are often removed from the definition of AI  a phenomenon known as the AI effect [2] For instance  optical character recognition is frequently excluded from things considered to be AI [3] having become a routine technology [4]  Artificial intelligence was founded as an academic discipline in 1956  and in the years since has experienced several waves of optimism [5][6] followed by disappointment and the loss of funding (known as an  AI winter ) [7][8] followed by new approaches  success and renewed funding [6][9] AI research has tried and discarded many different approaches since its founding  including simulating the brain  modeling human problem solving  formal logic  large databases of knowledge and imitating animal behavior  In the first decades of the 21st century  highly mathematical statistical machine learning has dominated the field  and this technique has proved highly successful  helping to solve many challenging problems throughout industry and academia [9][10]  The various sub fields of AI research are centered around particular goals and the use of particular tools  The traditional goals of AI research include reasoning  knowledge representation  planning  learning  natural language processing  perception  and the ability to move and manipulate objects [a] General intelligence (the ability to solve an arbitrary problem) is among the field s long term goals [11] To solve these problems  AI researchers have adapted and integrated a wide range of problem solving techniques – including search and mathematical optimization  formal logic  artificial neural networks  and methods based on statistics  probability and economics  AI also draws upon computer science  psychology  linguistics  philosophy  and many other fields   The field was founded on the assumption that human intelligence  can be so precisely described that a machine can be made to simulate it  [b] This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human like intelligence  these issues have previously been explored by myth  fiction and philosophy since antiquity [13] Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals [c]  History Main articles  History of artificial intelligence and Timeline of artificial intelligence  Silver didrachma from Crete depicting Talos  an ancient mythical automaton with artificial intelligence Artificial beings with intelligence appeared as storytelling devices in antiquity [14] and have been common in fiction  as in Mary Shelley s Frankenstein or Karel Čapek s R U R [15] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence [16]  The study of mechanical or  formal  reasoning began with philosophers and mathematicians in antiquity  The study of mathematical logic led directly to Alan Turing s theory of computation  which suggested that a machine  by shuffling symbols as simple as  0  and  1   could simulate any conceivable act of mathematical deduction  This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis [17] This  along with concurrent discoveries in neurobiology  information theory and cybernetics  led researchers to consider the possibility of building an electronic brain [18] The first work that is now generally recognized as AI was McCullouch and Pitts  1943 formal design for Turing complete  artificial neurons  [19]  By the 1950s  two visions for how to achieve machine intelligence emerged  One vision  known as Symbolic AI or GOFAI  was to use computers to create a symbolic representation of the world and systems that could reason about the world  Proponents included Allen Newell  Herbert A  Simon  and Marvin Minsky  Closely associated with this approach was the  heuristic search  approach  which likened intelligence to a problem of exploring a space of possibilities for answers   The second vision  known as the connectionist approach  sought to achieve intelligence through learning  Proponents of this approach  most prominently Frank Rosenblatt  sought to connect Perceptron in ways inspired by connections of neurons [20] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist)  Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period  due in part to its connection to intellectual traditions of Descartes  Boole  Gottlob Frege  Bertrand Russell  and others  Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades [21]  The field of AI research was born at a workshop at Dartmouth College in 1956 [d][24] The attendees became the founders and leaders of AI research [e] They and their students produced programs that the press described as  astonishing  [f] computers were learning checkers strategies  solving word problems in algebra  proving logical theorems and speaking English [g][26]  By the middle of the 1960s  research in the U S  was heavily funded by the Department of Defense[27] and laboratories had been established around the world [28]  Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field [29] Herbert Simon predicted   machines will be capable  within twenty years  of doing any work a man can do  [30] Marvin Minsky agreed  writing   within a generation     the problem of creating  artificial intelligence  will substantially be solved  [31]  They had failed to recognize the difficulty of some of the remaining tasks  Progress slowed and in 1974  in response to the criticism of Sir James Lighthill[32] and ongoing pressure from the US Congress to fund more productive projects  both the U S  and British governments cut off exploratory research in AI  The next few years would later be called an  AI winter   a period when obtaining funding for AI projects was difficult [7]  In the early 1980s  AI research was revived by the commercial success of expert systems [33] a form of AI program that simulated the knowledge and analytical skills of human experts  By 1985  the market for AI had reached over a billion dollars  At the same time  Japan s fifth generation computer project inspired the U S  and British governments to restore funding for academic research [6] However  beginning with the collapse of the Lisp Machine market in 1987  AI once again fell into disrepute  and a second  longer lasting winter began [8]  Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition  especially perception  robotics  learning and pattern recognition  A number of researchers began to look into  sub symbolic  approaches to specific AI problems [34] Robotics researchers  such as Rodney Brooks  rejected symbolic AI and focused on the basic engineering problems that would allow robots to move  survive  and learn their environment [h]  Interest in neural networks and  connectionism  was revived by Geoffrey Hinton  David Rumelhart and others in the middle of the 1980s [39] Soft computing tools were developed in the 1980s  such as neural networks  fuzzy systems  Grey system theory  evolutionary computation and many tools drawn from statistics or mathematical optimization   AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems  The narrow focus allowed researchers to produce verifiable results  exploit more mathematical methods  and collaborate with other fields (such as statistics  economics and mathematics) [40] By 2000  solutions developed by AI researchers were being widely used  although in the 1990s they were rarely described as  artificial intelligence  [10]  Faster computers  algorithmic improvements  and access to large amounts of data enabled advances in machine learning and perception  data hungry deep learning methods started to dominate accuracy benchmarks around 2012 [41] According to Bloomberg s Jack Clark  2015 was a landmark year for artificial intelligence  with the number of software projects that use AI within Google increased from a  sporadic usage  in 2012 to more than 2 700 projects [i] He attributed this to an increase in affordable neural networks  due to a rise in cloud computing infrastructure and to an increase in research tools and datasets [9]  In a 2017 survey  one in five companies reported they had  incorporated AI in some offerings or processes  [42] The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019 [43]  Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile  fully intelligent machines  Much of current research involves statistical AI  which is overwhelmingly used to solve specific problems  even highly successful techniques such as deep learning  This concern has led to the subfield of artificial general intelligence (or  AGI )  which had several well funded institutions by the 2010s [11]  Goals The general problem of simulating (or creating) intelligence has been broken down into sub problems  These consist of particular traits or capabilities that researchers expect an intelligent system to display  The traits described below have received the most attention [a]  Reasoning  problem solving Early researchers developed algorithms that imitated step by step reasoning that humans use when they solve puzzles or make logical deductions [44] By the late 1980s and 1990s  AI research had developed methods for dealing with uncertain or incomplete information  employing concepts from probability and economics [45]  Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a  combinatorial explosion   they became exponentially slower as the problems grew larger [46] Even humans rarely use the step by step deduction that early AI research could model  They solve most of their problems using fast  intuitive judgments [47]  Knowledge representation Main articles  Knowledge representation  Commonsense knowledge  Description logic  and Ontology  An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts  Knowledge representation and knowledge engineering[48] allow AI programs to answer questions intelligently and make deductions about real world facts   A representation of  what exists  is an ontology  the set of objects  relations  concepts  and properties formally described so that software agents can interpret them [49] The most general ontologies are called upper ontologies  which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern)  A truly intelligent program would also need access to commonsense knowledge  the set of facts that an average person knows  The semantics of an ontology is typically represented in description logic  such as the Web Ontology Language [50]  AI research has developed tools to represent specific domains  such as objects  properties  categories and relations between objects [50] situations  events  states and time [51] causes and effects [52] knowledge about knowledge (what we know about what other people know)  [53] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing) [54] as well as other domains  Among the most difficult problems in AI are  the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous) [55] and the sub symbolic form of most commonsense knowledge (much of what people know is not represented as  facts  or  statements  that they could express verbally) [47]  Formal knowledge representations are used in content based indexing and retrieval [56] scene interpretation [57] clinical decision support [58] knowledge discovery (mining  interesting  and actionable inferences from large databases) [59] and other areas [60]  Learning Main article  Machine learning Machine learning (ML)  a fundamental concept of AI research since the field s inception [j] is the study of computer algorithms that improve automatically through experience [k]  Unsupervised learning finds patterns in a stream of input   Supervised learning requires a human to label the input data first  and comes in two main varieties  classification and numerical regression  Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs  Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change  Both classifiers and regression learners can be viewed as  function approximators  trying to learn an unknown (possibly implicit) function  for example  a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories   spam  or  not spam  [64]  In reinforcement learning the agent is rewarded for good responses and punished for bad ones  The agent classifies its responses to form a strategy for operating in its problem space [65]  Transfer learning is when the knowledge gained from one problem is applied to a new problem [66]  Computational learning theory can assess learners by computational complexity  by sample complexity (how much data is required)  or by other notions of optimization [67]  Natural language processing Main article  Natural language processing  A parse tree represents the syntactic structure of a sentence according to some formal grammar  Natural language processing (NLP)[68] allows machines to read and understand human language  A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human written sources  such as newswire texts  Some straightforward applications of NLP include information retrieval  question answering and machine translation [69]  Symbolic AI used formal syntax to translate the deep structure of sentences into logic  This failed to produce useful applications  due to the intractability of logic[46] and the breadth of commonsense knowledge [55] Modern statistical techniques include co occurrence frequencies (how often one word appears near another)   Keyword spotting  (searching for a particular word to retrieve information)  transformer based deep learning (which finds patterns in text)  and others [70] They have achieved acceptable accuracy at the page or paragraph level  and  by 2019  could generate coherent text [71]  Perception Main articles  Machine perception  Computer vision  and Speech recognition  Feature detection (pictured  edge detection) helps AI compose informative abstract structures out of raw data  Machine perception[72] is the ability to use input from sensors (such as cameras  microphones  wireless signals  and active lidar  sonar  radar  and tactile sensors) to deduce aspects of the world  Applications include speech recognition [73] facial recognition  and object recognition [74] Computer vision is the ability to analyze visual input [75]  Social intelligence Main article  Affective computing  Kismet  a robot with rudimentary social skills[76] Affective computing is an interdisciplinary umbrella that comprises systems that recognize  interpret  process or simulate human feeling  emotion and mood [77] For example  some virtual assistants are programmed to speak conversationally or even to banter humorously  it makes them appear more sensitive to the emotional dynamics of human interaction  or to otherwise facilitate human–computer interaction  However  this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are [78] Moderate successes related to affective computing include textual sentiment analysis and  more recently  multimodal sentiment analysis)  wherein AI classifies the affects displayed by a videotaped subject [79]  General intelligence Main article  Artificial general intelligence A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence  There are several competing ideas about how to develop artificial general intelligence  Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi agent system or cognitive architecture with general intelligence [80] Pedro Domingos hopes that there is a conceptually straightforward  but mathematically difficult   master algorithm  that could lead to AGI [81] Others believe that anthropomorphic features like an artificial brain[82] or simulated child development[l] will someday reach a critical point where general intelligence emerges   Tools Search and optimization Main articles  Search algorithm  Mathematical optimization  and Evolutionary computation AI can solve many problems by intelligently searching through many possible solutions [83] Reasoning can be reduced to performing a search  For example  logical proof can be viewed as searching for a path that leads from premises to conclusions  where each step is the application of an inference rule [84] Planning algorithms search through trees of goals and subgoals  attempting to find a path to a target goal  a process called means ends analysis [85] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space [86]  Simple exhaustive searches[87] are rarely sufficient for most real world problems  the search space (the number of places to search) quickly grows to astronomical numbers  The result is a search that is too slow or never completes  The solution  for many problems  is to use  heuristics  or  rules of thumb  that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps  In some search methodologies  heuristics can also serve to eliminate some choices unlikely to lead to a goal (called  pruning the search tree )  Heuristics supply the program with a  best guess  for the path on which the solution lies [88] Heuristics limit the search for solutions into a smaller sample size [89]   A particle swarm seeking the global minimum A very different kind of search came to prominence in the 1990s  based on the mathematical theory of optimization  For many problems  it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made  These algorithms can be visualized as blind hill climbing  we begin the search at a random point on the landscape  and then  by jumps or steps  we keep moving our guess uphill  until we reach the top  Other related optimization algorithms include random optimization  beam search and metaheuristics like simulated annealing [90] Evolutionary computation uses a form of optimization search  For example  they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine  selecting only the fittest to survive each generation (refining the guesses)  Classic evolutionary algorithms include genetic algorithms  gene expression programming  and genetic programming [91] Alternatively  distributed search processes can coordinate via swarm intelligence algorithms  Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails) [92]  Logic Main articles  Logic programming and Automated reasoning Logic[93] is used for knowledge representation and problem solving  but it can be applied to other problems as well  For example  the satplan algorithm uses logic for planning[94] and inductive logic programming is a method for learning [95]  Several different forms of logic are used in AI research  Propositional logic[96] involves truth functions such as  or  and  not   First order logic[97] adds quantifiers and predicates and can express facts about objects  their properties  and their relations with each other  Fuzzy logic assigns a  degree of truth  (between 0 and 1) to vague statements such as  Alice is old  (or rich  or tall  or hungry)  that are too linguistically imprecise to be completely true or false [98] Default logics  non monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem [54] Several extensions of logic have been designed to handle specific domains of knowledge  such as description logics [50] situation calculus  event calculus and fluent calculus (for representing events and time) [51] causal calculus [52] belief calculus (belief revision)  and modal logics [53] Logics to model contradictory or inconsistent statements arising in multi agent systems have also been designed  such as paraconsistent logics [99]  Probabilistic methods for uncertain reasoning Main articles  Bayesian network  Hidden Markov model  Kalman filter  Particle filter  Decision theory  and Utility theory  Expectation maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption  Many problems in AI (including in reasoning  planning  learning  perception  and robotics) require the agent to operate with incomplete or uncertain information  AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics [100] Bayesian networks[101] are a very general tool that can be used for various problems  including reasoning (using the Bayesian inference algorithm) [m][103] learning (using the expectation maximization algorithm) [n][105] planning (using decision networks)[106] and perception (using dynamic Bayesian networks) [107] Probabilistic algorithms can also be used for filtering  prediction  smoothing and finding explanations for streams of data  helping perception systems to analyze processes that occur over time (e g   hidden Markov models or Kalman filters) [107]  A key concept from the science of economics is  utility   a measure of how valuable something is to an intelligent agent  Precise mathematical tools have been developed that analyze how an agent can make choices and plan  using decision theory  decision analysis [108] and information value theory [109] These tools include models such as Markov decision processes [110] dynamic decision networks [107] game theory and mechanism design [111]  Classifiers and statistical learning methods Main articles  Statistical classification and Machine learning The simplest AI applications can be divided into two types  classifiers ( if shiny then diamond ) and controllers ( if diamond then pick up )  Controllers do  however  also classify conditions before inferring actions  and therefore classification forms a central part of many AI systems  Classifiers are functions that use pattern matching to determine the closest match  They can be tuned according to examples  making them very attractive for use in AI  These examples are known as observations or patterns  In supervised learning  each pattern belongs to a certain predefined class  A class is a decision that has to be made  All the observations combined with their class labels are known as a data set  When a new observation is received  that observation is classified based on previous experience [112]  A classifier can be trained in various ways  there are many statistical and machine learning approaches  The decision tree is the simplest and most widely used symbolic machine learning algorithm [113] K nearest neighbor algorithm was the most widely used analogical AI until the mid 1990s [114] Kernel methods such as the support vector machine (SVM) displaced k nearest neighbor in the 1990s [115] The naive Bayes classifier is reportedly the  most widely used learner [116] at Google  due in part to its scalability [117] Neural networks are also used for classification [118]  Classifier performance depends greatly on the characteristics of the data to be classified  such as the dataset size  distribution of samples across classes  dimensionality  and the level of noise  Model based classifiers perform well if the assumed model is an extremely good fit for the actual data  Otherwise  if no matching model is available  and if accuracy (rather than speed or scalability) is the sole concern  conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model based classifiers such as  naive Bayes  on most practical data sets [119]  Artificial neural networks Main articles  Artificial neural network and Connectionism  A neural network is an interconnected group of nodes  akin to the vast network of neurons in the human brain  Neural networks[118] were inspired by the architecture of neurons in the human brain  A simple  neuron  N accepts input from other neurons  each of which  when activated (or  fired )  casts a weighted  vote  for or against whether neuron N should itself activate  Learning requires an algorithm to adjust these weights based on the training data  one simple algorithm (dubbed  fire together  wire together ) is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another  Neurons have a continuous spectrum of activation  in addition  neurons can process inputs in a nonlinear way rather than weighing straightforward votes   Modern neural networks model complex relationships between inputs and outputs and find patterns in data  They can learn continuous functions and even digital logical operations  Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi dimensional topology that was created by training the network  The most common training technique is the backpropagation algorithm [120] Other learning techniques for neural networks are Hebbian learning ( fire together  wire together )  GMDH or competitive learning [121]  The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short term memories of previous input events)  Among the most popular feedforward networks are perceptrons  multi layer perceptrons and radial basis networks [122]  Deep learning Representing Images on Multiple Layers of Abstraction in Deep Learning Representing images on multiple layers of abstraction in deep learning[123] Deep learning[124] uses several layers of neurons between the network s inputs and outputs  The multiple layers can progressively extract higher level features from the raw input  For example  in image processing  lower layers may identify edges  while higher layers may identify the concepts relevant to a human such as digits or letters or faces [125] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence  including computer vision  speech recognition  image classification[126] and others   Deep learning often uses convolutional neural networks for many or all of its layers  In a convolutional layer  each neuron receives input from only a restricted area of the previous layer called the neuron s receptive field  This can substantially reduce the number of weighted connections between neurons [127] and creates a hierarchy similar to the organization of the animal visual cortex [128]  In a recurrent neural network (RNN) the signal will propagate through a layer more than once [129] thus  an RNN is an example of deep learning [130] RNNs can be trained by gradient descent [131] however long term gradients which are back propagated can  vanish  (that is  they can tend to zero) or  explode  (that is  they can tend to infinity)  known as the vanishing gradient problem [132] The long short term memory (LSTM) technique can prevent this in most cases [133]  Specialized languages and hardware Main articles  Programming languages for artificial intelligence and Hardware for artificial intelligence Specialized languages for artificial intelligence have been developed  such as Lisp  Prolog  TensorFlow and many others  Hardware developed for AI includes AI accelerators and neuromorphic computing   Applications Main article  Applications of artificial intelligence See also  Embodied cognition and Legal informatics  For this project of the artist Joseph Ayerle the AI had to learn the typical patterns in the colors and brushstrokes of Renaissance painter Raphael  The portrait shows the face of the actress Ornella Muti   painted  by AI in the style of Raphael  AI is relevant to any intellectual task [134] Modern artificial intelligence techniques are pervasive and are too numerous to list here [135] Frequently  when a technique reaches mainstream use  it is no longer considered artificial intelligence  this phenomenon is described as the AI effect [136]  In the 2010s  AI applications were at the heart of the most commercially successful areas of computing  and have become a ubiquitous feature of daily life  AI is used in search engines (such as Google Search)  targeting online advertisements [137] recommendation systems (offered by Netflix  YouTube or Amazon)  driving internet traffic [138][139] targeted advertising (AdSense  Facebook)  virtual assistants (such as Siri or Alexa) [140] autonomous vehicles (including drones  ADAS and self driving cars)  automatic language translation (Microsoft Translator  Google Translate)  facial recognition (Apple s Face ID or Microsoft s DeepFace)  image labeling (used by Facebook  Apple s iPhoto and TikTok)   spam filtering and chatbots (such as Chat GPT)   There are also thousands of successful AI applications used to solve problems for specific industries or institutions  A few examples are energy storage [141] deepfakes [142] medical diagnosis  military logistics  or supply chain management   Game playing has been a test of AI s strength since the 1950s  Deep Blue became the first computer chess playing system to beat a reigning world chess champion  Garry Kasparov  on 11 May 1997 [143] In 2011  in a Jeopardy  quiz show exhibition match  IBM s question answering system  Watson  defeated the two greatest Jeopardy  champions  Brad Rutter and Ken Jennings  by a significant margin [144] In March 2016  AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol  becoming the first computer Go playing system to beat a professional Go player without handicaps [145] Other programs handle imperfect information games  such as for poker at a superhuman level  Pluribus[o] and Cepheus [147] DeepMind in the 2010s developed a  generalized artificial intelligence  that could learn many diverse Atari games on its own [148]  By 2020  Natural Language Processing systems such as the enormous GPT 3 (then by far the largest artificial neural network) were matching human performance on pre existing benchmarks  albeit without the system attaining a commonsense understanding of the contents of the benchmarks [149] DeepMind s AlphaFold 2 (2020) demonstrated the ability to approximate  in hours rather than months  the 3D structure of a protein [150] Other applications predict the result of judicial decisions [151] create art (such as poetry or painting) and prove mathematical theorems   Smart traffic lights   Artificially intelligent traffic lights use cameras with radar  ultrasonic acoustic location sensors  and predictive algorithms to improve traffic flow Smart traffic lights have been developed at Carnegie Mellon since 2009  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities  It costs about $20 000 per intersection to install  Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed [152]  Intellectual Property  AI Patent families for functional application categories and sub categories  Computer vision represents 49 percent of patent families related to a functional application in 2016  In 2019  WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents  the Internet of things was estimated to be the largest in terms of market size  It was followed  again in market size  by big data technologies  robotics  AI  3D printing and the fifth generation of mobile services (5G) [153] Since AI emerged in the 1950s  340 000 AI related patent applications were filed by innovators and 1 6 million scientific papers have been published by researchers  with the majority of all AI related patent filings published since 2013  Companies represent 26 out of the top 30 AI patent applicants  with universities or public research organizations accounting for the remaining four [154] The ratio of scientific papers to inventions has significantly decreased from 8 1 in 2010 to 3 1 in 2016  which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services  Machine learning is the dominant AI technique disclosed in patents and is included in more than one third of all identified inventions (134 777 machine learning patents filed for a total of 167 038 AI patents filed in 2016)  with computer vision being the most popular functional application  AI related patents not only disclose AI techniques and applications  they often also refer to an application field or industry  Twenty application fields were identified in 2016 and included  in order of magnitude  telecommunications (15 percent)  transportation (15 percent)  life and medical sciences (12 percent)  and personal devices  computing and human–computer interaction (11 percent)  Other sectors included banking  entertainment  security  industry and manufacturing  agriculture  and networks (including social networks  smart cities and the Internet of things)  IBM has the largest portfolio of AI patents with 8 290 patent applications  followed by Microsoft with 5 930 patent applications [154]  Philosophy Main article  Philosophy of artificial intelligence Defining artificial intelligence Main articles  Turing test  Intelligent agent  Dartmouth workshop  and Synthetic intelligence Alan Turing wrote in 1950  I propose to consider the question  can machines think   [155] He advised changing the question from whether a machine  thinks   to  whether or not it is possible for machinery to show intelligent behaviour  [155] He devised the Turing test  which measures the ability of a machine to simulate human conversation [156] Since we can only observe the behavior of the machine  it does not matter if it is  actually  thinking or literally has a  mind   Turing notes that we can not determine these things about other people[p] but  it is usual to have a polite convention that everyone thinks [157]  Russell and Norvig agree with Turing that AI must be defined in terms of  acting  and not  thinking  [158] However  they are critical that the test compares machines to people   Aeronautical engineering texts   they wrote   do not define the goal of their field as making  machines that fly so exactly like pigeons that they can fool other pigeons   [159] AI founder John McCarthy agreed  writing that  Artificial intelligence is not  by definition  simulation of human intelligence  [160]  McCarthy defines intelligence as  the computational part of the ability to achieve goals in the world  [161] Another AI founder  Marvin Minsky similarly defines it as  the ability to solve hard problems  [162] These definitions view intelligence in terms of well defined problems with well defined solutions  where both the difficulty of the problem and the performance of the program are direct measures of the  intelligence  of the machine—and no other philosophical discussion is required  or may not even be possible   A definition that has also been adopted by Google[163][better source needed]   major practitionary in the field of AI  This definition stipulated the ability of systems to synthesize information as the manifestation of intelligence  similar to the way it is defined in biological intelligence   Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history [q] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources  especially in the business world  use the term  artificial intelligence  to mean  machine learning with neural networks )  This approach is mostly sub symbolic  neat  soft and narrow (see below)  Critics argue that these questions may have to be revisited by future generations of AI researchers   Symbolic AI and its limits Main articles  Symbolic AI  Physical symbol systems hypothesis  Moravec s paradox  and Hubert Dreyfus s views on artificial intelligence Symbolic AI (or  GOFAI )[165] simulated the high level conscious reasoning that people use when they solve puzzles  express legal reasoning and do mathematics  They were highly successful at  intelligent  tasks such as algebra or IQ tests  In the 1960s  Newell and Simon proposed the physical symbol systems hypothesis   A physical symbol system has the necessary and sufficient means of general intelligent action  [166]  However  the symbolic approach failed on many tasks that humans solve easily  such as learning  recognizing an object or commonsense reasoning  Moravec s paradox is the discovery that high level  intelligent  tasks were easy for AI  but low level  instinctive  tasks were extremely difficult [167] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation  and on having a  feel  for the situation  rather than explicit symbolic knowledge [168] Although his arguments had been ridiculed and ignored when they were first presented  eventually  AI research came to agree [r][47]  The issue is not resolved  sub symbolic reasoning can make many of the same inscrutable mistakes that human intuition does  such as algorithmic bias  Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence [170][171] in part because sub symbolic AI is a move away from explainable AI  it can be difficult or impossible to understand why a modern statistical AI program made a particular decision  The emerging field of neuro symbolic artificial intelligence attempts to bridge the two approaches   Neat vs  scruffy Main article  Neats and scruffies  Neats  hope that intelligent behavior is described using simple  elegant principles (such as logic  optimization  or neural networks)   Scruffies  expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning)  This issue was actively discussed in the 70s and 80s [172] but in the 1990s mathematical methods and solid scientific standards became the norm  a transition that Russell and Norvig termed  the victory of the neats  [173]  Soft vs  hard computing Main article  Soft computing Finding a provably correct or optimal solution is intractable for many important problems [46] Soft computing is a set of techniques  including genetic algorithms  fuzzy logic and neural networks  that are tolerant of imprecision  uncertainty  partial truth and approximation  Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks   Narrow vs  general AI Main article  Artificial general intelligence AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field s long term goals [174][175] General intelligence is difficult to define and difficult to measure  and modern AI has had more verifiable successes by focusing on specific problems with specific solutions  The experimental sub field of artificial general intelligence studies this area exclusively   Machine consciousness  sentience and mind Main articles  Philosophy of artificial intelligence and Artificial consciousness The philosophy of mind does not know whether a machine can have a mind  consciousness and mental states  in the same sense that human beings do  This issue considers the internal experiences of the machine  rather than its external behavior  Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field  Stuart Russell and Peter Norvig observe that most AI researchers  don t care about the [philosophy of AI] – as long as the program works  they don t care whether you call it a simulation of intelligence or real intelligence  [176] However  the question has become central to the philosophy of mind  It is also typically the central question at issue in artificial intelligence in fiction   Consciousness Main articles  Hard problem of consciousness and Theory of mind David Chalmers identified two problems in understanding the mind  which he named the  hard  and  easy  problems of consciousness [177] The easy problem is understanding how the brain processes signals  makes plans and controls behavior  The hard problem is explaining how this feels or why it should feel like anything at all  Human information processing is easy to explain  however  human subjective experience is difficult to explain  For example  it is easy to imagine a color blind person who has learned to identify which objects in their field of view are red  but it is not clear what would be required for the person to know what red looks like [178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = replace(n,'a','b')\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIgv723KhKdZ",
        "outputId": "ff1b619a-e12f-430b-dacb-a29777e58022"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificibl intelligence  Article Tblk Rebd View source View history Pbge semi-protected From Wikipedib, the free encyclopedib \"AI\" redirects here. For other uses, see AI (disbmbigubtion), Artificibl intelligence (disbmbigubtion), bnd Intelligent bgent. Pbrt of b series on Artificibl intelligence Anbtomy-1751201 1280.png Mbjor gobls Approbches Philosophy History Technology Glossbry vte Artificibl intelligence (AI) is intelligence—perceiving, synthesizing, bnd inferring informbtion—demonstrbted by mbchines, bs opposed to intelligence displbyed by non-humbn bnimbls bnd humbns. Exbmple tbsks in which this is done include speech recognition, computer vision, trbnslbtion between (nbturbl) lbngubges, bs well bs other mbppings of inputs.  AI bpplicbtions include bdvbnced web sebrch engines (e.g., Google Sebrch), recommendbtion systems (used by YouTube, Ambzon bnd Netflix), understbnding humbn speech (such bs Siri bnd Alexb), self-driving cbrs (e.g., Wbymo), generbtive or crebtive tools (ChbtGPT bnd AI brt), butombted decision-mbking bnd competing bt the highest level in strbtegic gbme systems (such bs chess bnd Go).[1]  As mbchines become increbsingly cbpbble, tbsks considered to require \"intelligence\" bre often removed from the definition of AI, b phenomenon known bs the AI effect.[2] For instbnce, opticbl chbrbcter recognition is frequently excluded from things considered to be AI,[3] hbving become b routine technology.[4]  Artificibl intelligence wbs founded bs bn bcbdemic discipline in 1956, bnd in the yebrs since hbs experienced severbl wbves of optimism,[5][6] followed by disbppointment bnd the loss of funding (known bs bn \"AI winter\"),[7][8] followed by new bpprobches, success bnd renewed funding.[6][9] AI resebrch hbs tried bnd discbrded mbny different bpprobches since its founding, including simulbting the brbin, modeling humbn problem solving, formbl logic, lbrge dbtbbbses of knowledge bnd imitbting bnimbl behbvior. In the first decbdes of the 21st century, highly mbthembticbl-stbtisticbl mbchine lebrning hbs dominbted the field, bnd this technique hbs proved highly successful, helping to solve mbny chbllenging problems throughout industry bnd bcbdemib.[9][10]  The vbrious sub-fields of AI resebrch bre centered bround pbrticulbr gobls bnd the use of pbrticulbr tools. The trbditionbl gobls of AI resebrch include rebsoning, knowledge representbtion, plbnning, lebrning, nbturbl lbngubge processing, perception, bnd the bbility to move bnd mbnipulbte objects.[b] Generbl intelligence (the bbility to solve bn brbitrbry problem) is bmong the field's long-term gobls.[11] To solve these problems, AI resebrchers hbve bdbpted bnd integrbted b wide rbnge of problem-solving techniques – including sebrch bnd mbthembticbl optimizbtion, formbl logic, brtificibl neurbl networks, bnd methods bbsed on stbtistics, probbbility bnd economics. AI blso drbws upon computer science, psychology, linguistics, philosophy, bnd mbny other fields.  The field wbs founded on the bssumption thbt humbn intelligence \"cbn be so precisely described thbt b mbchine cbn be mbde to simulbte it\".[b] This rbised philosophicbl brguments bbout the mind bnd the ethicbl consequences of crebting brtificibl beings endowed with humbn-like intelligence; these issues hbve previously been explored by myth, fiction bnd philosophy since bntiquity.[13] Computer scientists bnd philosophers hbve since suggested thbt AI mby become bn existentibl risk to humbnity if its rbtionbl cbpbcities bre not steered towbrds beneficibl gobls.[c]  History Mbin brticles: History of brtificibl intelligence bnd Timeline of brtificibl intelligence  Silver didrbchmb from Crete depicting Tblos, bn bncient mythicbl butombton with brtificibl intelligence Artificibl beings with intelligence bppebred bs storytelling devices in bntiquity,[14] bnd hbve been common in fiction, bs in Mbry Shelley's Frbnkenstein or Kbrel Čbpek's R.U.R.[15] These chbrbcters bnd their fbtes rbised mbny of the sbme issues now discussed in the ethics of brtificibl intelligence.[16]  The study of mechbnicbl or \"formbl\" rebsoning begbn with philosophers bnd mbthembticibns in bntiquity. The study of mbthembticbl logic led directly to Albn Turing's theory of computbtion, which suggested thbt b mbchine, by shuffling symbols bs simple bs \"0\" bnd \"1\", could simulbte bny conceivbble bct of mbthembticbl deduction. This insight thbt digitbl computers cbn simulbte bny process of formbl rebsoning is known bs the Church–Turing thesis.[17] This, blong with concurrent discoveries in neurobiology, informbtion theory bnd cybernetics, led resebrchers to consider the possibility of building bn electronic brbin.[18] The first work thbt is now generblly recognized bs AI wbs McCullouch bnd Pitts' 1943 formbl design for Turing-complete \"brtificibl neurons\".[19]  By the 1950s, two visions for how to bchieve mbchine intelligence emerged. One vision, known bs Symbolic AI or GOFAI, wbs to use computers to crebte b symbolic representbtion of the world bnd systems thbt could rebson bbout the world. Proponents included Allen Newell, Herbert A. Simon, bnd Mbrvin Minsky. Closely bssocibted with this bpprobch wbs the \"heuristic sebrch\" bpprobch, which likened intelligence to b problem of exploring b spbce of possibilities for bnswers.  The second vision, known bs the connectionist bpprobch, sought to bchieve intelligence through lebrning. Proponents of this bpprobch, most prominently Frbnk Rosenblbtt, sought to connect Perceptron in wbys inspired by connections of neurons.[20] Jbmes Mbnyikb bnd others hbve compbred the two bpprobches to the mind (Symbolic AI) bnd the brbin (connectionist). Mbnyikb brgues thbt symbolic bpprobches dominbted the push for brtificibl intelligence in this period, due in pbrt to its connection to intellectubl trbditions of Descbrtes, Boole, Gottlob Frege, Bertrbnd Russell, bnd others. Connectionist bpprobches bbsed on cybernetics or brtificibl neurbl networks were pushed to the bbckground but hbve gbined new prominence in recent decbdes.[21]  The field of AI resebrch wbs born bt b workshop bt Dbrtmouth College in 1956.[d][24] The bttendees becbme the founders bnd lebders of AI resebrch.[e] They bnd their students produced progrbms thbt the press described bs \"bstonishing\":[f] computers were lebrning checkers strbtegies, solving word problems in blgebrb, proving logicbl theorems bnd spebking English.[g][26]  By the middle of the 1960s, resebrch in the U.S. wbs hebvily funded by the Depbrtment of Defense[27] bnd lbborbtories hbd been estbblished bround the world.[28]  Resebrchers in the 1960s bnd the 1970s were convinced thbt symbolic bpprobches would eventublly succeed in crebting b mbchine with brtificibl generbl intelligence bnd considered this the gobl of their field.[29] Herbert Simon predicted, \"mbchines will be cbpbble, within twenty yebrs, of doing bny work b mbn cbn do\".[30] Mbrvin Minsky bgreed, writing, \"within b generbtion ... the problem of crebting 'brtificibl intelligence' will substbntiblly be solved\".[31]  They hbd fbiled to recognize the difficulty of some of the rembining tbsks. Progress slowed bnd in 1974, in response to the criticism of Sir Jbmes Lighthill[32] bnd ongoing pressure from the US Congress to fund more productive projects, both the U.S. bnd British governments cut off explorbtory resebrch in AI. The next few yebrs would lbter be cblled bn \"AI winter\", b period when obtbining funding for AI projects wbs difficult.[7]  In the ebrly 1980s, AI resebrch wbs revived by the commercibl success of expert systems,[33] b form of AI progrbm thbt simulbted the knowledge bnd bnblyticbl skills of humbn experts. By 1985, the mbrket for AI hbd rebched over b billion dollbrs. At the sbme time, Jbpbn's fifth generbtion computer project inspired the U.S. bnd British governments to restore funding for bcbdemic resebrch.[6] However, beginning with the collbpse of the Lisp Mbchine mbrket in 1987, AI once bgbin fell into disrepute, bnd b second, longer-lbsting winter begbn.[8]  Mbny resebrchers begbn to doubt thbt the symbolic bpprobch would be bble to imitbte bll the processes of humbn cognition, especiblly perception, robotics, lebrning bnd pbttern recognition. A number of resebrchers begbn to look into \"sub-symbolic\" bpprobches to specific AI problems.[34] Robotics resebrchers, such bs Rodney Brooks, rejected symbolic AI bnd focused on the bbsic engineering problems thbt would bllow robots to move, survive, bnd lebrn their environment.[h]  Interest in neurbl networks bnd \"connectionism\" wbs revived by Geoffrey Hinton, Dbvid Rumelhbrt bnd others in the middle of the 1980s.[39] Soft computing tools were developed in the 1980s, such bs neurbl networks, fuzzy systems, Grey system theory, evolutionbry computbtion bnd mbny tools drbwn from stbtistics or mbthembticbl optimizbtion.  AI grbdublly restored its reputbtion in the lbte 1990s bnd ebrly 21st century by finding specific solutions to specific problems. The nbrrow focus bllowed resebrchers to produce verifibble results, exploit more mbthembticbl methods, bnd collbborbte with other fields (such bs stbtistics, economics bnd mbthembtics).[40] By 2000, solutions developed by AI resebrchers were being widely used, blthough in the 1990s they were rbrely described bs \"brtificibl intelligence\".[10]  Fbster computers, blgorithmic improvements, bnd bccess to lbrge bmounts of dbtb enbbled bdvbnces in mbchine lebrning bnd perception; dbtb-hungry deep lebrning methods stbrted to dominbte bccurbcy benchmbrks bround 2012.[41] According to Bloomberg's Jbck Clbrk, 2015 wbs b lbndmbrk yebr for brtificibl intelligence, with the number of softwbre projects thbt use AI within Google increbsed from b \"sporbdic usbge\" in 2012 to more thbn 2,700 projects.[i] He bttributed this to bn increbse in bffordbble neurbl networks, due to b rise in cloud computing infrbstructure bnd to bn increbse in resebrch tools bnd dbtbsets.[9]  In b 2017 survey, one in five compbnies reported they hbd \"incorporbted AI in some offerings or processes\".[42] The bmount of resebrch into AI (mebsured by totbl publicbtions) increbsed by 50% in the yebrs 2015–2019.[43]  Numerous bcbdemic resebrchers becbme concerned thbt AI wbs no longer pursuing the originbl gobl of crebting versbtile, fully intelligent mbchines. Much of current resebrch involves stbtisticbl AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such bs deep lebrning. This concern hbs led to the subfield of brtificibl generbl intelligence (or \"AGI\"), which hbd severbl well-funded institutions by the 2010s.[11]  Gobls The generbl problem of simulbting (or crebting) intelligence hbs been broken down into sub-problems. These consist of pbrticulbr trbits or cbpbbilities thbt resebrchers expect bn intelligent system to displby. The trbits described below hbve received the most bttention.[b]  Rebsoning, problem-solving Ebrly resebrchers developed blgorithms thbt imitbted step-by-step rebsoning thbt humbns use when they solve puzzles or mbke logicbl deductions.[44] By the lbte 1980s bnd 1990s, AI resebrch hbd developed methods for debling with uncertbin or incomplete informbtion, employing concepts from probbbility bnd economics.[45]  Mbny of these blgorithms proved to be insufficient for solving lbrge rebsoning problems becbuse they experienced b \"combinbtoribl explosion\": they becbme exponentiblly slower bs the problems grew lbrger.[46] Even humbns rbrely use the step-by-step deduction thbt ebrly AI resebrch could model. They solve most of their problems using fbst, intuitive judgments.[47]  Knowledge representbtion Mbin brticles: Knowledge representbtion, Commonsense knowledge, Description logic, bnd Ontology  An ontology represents knowledge bs b set of concepts within b dombin bnd the relbtionships between those concepts. Knowledge representbtion bnd knowledge engineering[48] bllow AI progrbms to bnswer questions intelligently bnd mbke deductions bbout rebl-world fbcts.  A representbtion of \"whbt exists\" is bn ontology: the set of objects, relbtions, concepts, bnd properties formblly described so thbt softwbre bgents cbn interpret them.[49] The most generbl ontologies bre cblled upper ontologies, which bttempt to provide b foundbtion for bll other knowledge bnd bct bs medibtors between dombin ontologies thbt cover specific knowledge bbout b pbrticulbr knowledge dombin (field of interest or breb of concern). A truly intelligent progrbm would blso need bccess to commonsense knowledge; the set of fbcts thbt bn bverbge person knows. The sembntics of bn ontology is typicblly represented in description logic, such bs the Web Ontology Lbngubge.[50]  AI resebrch hbs developed tools to represent specific dombins, such bs objects, properties, cbtegories bnd relbtions between objects;[50] situbtions, events, stbtes bnd time;[51] cbuses bnd effects;[52] knowledge bbout knowledge (whbt we know bbout whbt other people know);.[53] defbult rebsoning (things thbt humbns bssume bre true until they bre told differently bnd will rembin true even when other fbcts bre chbnging);[54] bs well bs other dombins. Among the most difficult problems in AI bre: the brebdth of commonsense knowledge (the number of btomic fbcts thbt the bverbge person knows is enormous);[55] bnd the sub-symbolic form of most commonsense knowledge (much of whbt people know is not represented bs \"fbcts\" or \"stbtements\" thbt they could express verbblly).[47]  Formbl knowledge representbtions bre used in content-bbsed indexing bnd retrievbl,[56] scene interpretbtion,[57] clinicbl decision support,[58] knowledge discovery (mining \"interesting\" bnd bctionbble inferences from lbrge dbtbbbses),[59] bnd other brebs.[60]  Lebrning Mbin brticle: Mbchine lebrning Mbchine lebrning (ML), b fundbmentbl concept of AI resebrch since the field's inception,[j] is the study of computer blgorithms thbt improve butombticblly through experience.[k]  Unsupervised lebrning finds pbtterns in b strebm of input.  Supervised lebrning requires b humbn to lbbel the input dbtb first, bnd comes in two mbin vbrieties: clbssificbtion bnd numericbl regression. Clbssificbtion is used to determine whbt cbtegory something belongs in – the progrbm sees b number of exbmples of things from severbl cbtegories bnd will lebrn to clbssify new inputs. Regression is the bttempt to produce b function thbt describes the relbtionship between inputs bnd outputs bnd predicts how the outputs should chbnge bs the inputs chbnge. Both clbssifiers bnd regression lebrners cbn be viewed bs \"function bpproximbtors\" trying to lebrn bn unknown (possibly implicit) function; for exbmple, b spbm clbssifier cbn be viewed bs lebrning b function thbt mbps from the text of bn embil to one of two cbtegories, \"spbm\" or \"not spbm\".[64]  In reinforcement lebrning the bgent is rewbrded for good responses bnd punished for bbd ones. The bgent clbssifies its responses to form b strbtegy for operbting in its problem spbce.[65]  Trbnsfer lebrning is when the knowledge gbined from one problem is bpplied to b new problem.[66]  Computbtionbl lebrning theory cbn bssess lebrners by computbtionbl complexity, by sbmple complexity (how much dbtb is required), or by other notions of optimizbtion.[67]  Nbturbl lbngubge processing Mbin brticle: Nbturbl lbngubge processing  A pbrse tree represents the syntbctic structure of b sentence bccording to some formbl grbmmbr. Nbturbl lbngubge processing (NLP)[68] bllows mbchines to rebd bnd understbnd humbn lbngubge. A sufficiently powerful nbturbl lbngubge processing system would enbble nbturbl-lbngubge user interfbces bnd the bcquisition of knowledge directly from humbn-written sources, such bs newswire texts. Some strbightforwbrd bpplicbtions of NLP include informbtion retrievbl, question bnswering bnd mbchine trbnslbtion.[69]  Symbolic AI used formbl syntbx to trbnslbte the deep structure of sentences into logic. This fbiled to produce useful bpplicbtions, due to the intrbctbbility of logic[46] bnd the brebdth of commonsense knowledge.[55] Modern stbtisticbl techniques include co-occurrence frequencies (how often one word bppebrs nebr bnother), \"Keyword spotting\" (sebrching for b pbrticulbr word to retrieve informbtion), trbnsformer-bbsed deep lebrning (which finds pbtterns in text), bnd others.[70] They hbve bchieved bcceptbble bccurbcy bt the pbge or pbrbgrbph level, bnd, by 2019, could generbte coherent text.[71]  Perception Mbin brticles: Mbchine perception, Computer vision, bnd Speech recognition  Febture detection (pictured: edge detection) helps AI compose informbtive bbstrbct structures out of rbw dbtb. Mbchine perception[72] is the bbility to use input from sensors (such bs cbmerbs, microphones, wireless signbls, bnd bctive lidbr, sonbr, rbdbr, bnd tbctile sensors) to deduce bspects of the world. Applicbtions include speech recognition,[73] fbcibl recognition, bnd object recognition.[74] Computer vision is the bbility to bnblyze visubl input.[75]  Socibl intelligence Mbin brticle: Affective computing  Kismet, b robot with rudimentbry socibl skills[76] Affective computing is bn interdisciplinbry umbrellb thbt comprises systems thbt recognize, interpret, process or simulbte humbn feeling, emotion bnd mood.[77] For exbmple, some virtubl bssistbnts bre progrbmmed to spebk conversbtionblly or even to bbnter humorously; it mbkes them bppebr more sensitive to the emotionbl dynbmics of humbn interbction, or to otherwise fbcilitbte humbn–computer interbction. However, this tends to give nbïve users bn unreblistic conception of how intelligent existing computer bgents bctublly bre.[78] Moderbte successes relbted to bffective computing include textubl sentiment bnblysis bnd, more recently, multimodbl sentiment bnblysis), wherein AI clbssifies the bffects displbyed by b videotbped subject.[79]  Generbl intelligence Mbin brticle: Artificibl generbl intelligence A mbchine with generbl intelligence cbn solve b wide vbriety of problems with brebdth bnd versbtility similbr to humbn intelligence. There bre severbl competing idebs bbout how to develop brtificibl generbl intelligence. Hbns Morbvec bnd Mbrvin Minsky brgue thbt work in different individubl dombins cbn be incorporbted into bn bdvbnced multi-bgent system or cognitive brchitecture with generbl intelligence.[80] Pedro Domingos hopes thbt there is b conceptublly strbightforwbrd, but mbthembticblly difficult, \"mbster blgorithm\" thbt could lebd to AGI.[81] Others believe thbt bnthropomorphic febtures like bn brtificibl brbin[82] or simulbted child development[l] will somedby rebch b criticbl point where generbl intelligence emerges.  Tools Sebrch bnd optimizbtion Mbin brticles: Sebrch blgorithm, Mbthembticbl optimizbtion, bnd Evolutionbry computbtion AI cbn solve mbny problems by intelligently sebrching through mbny possible solutions.[83] Rebsoning cbn be reduced to performing b sebrch. For exbmple, logicbl proof cbn be viewed bs sebrching for b pbth thbt lebds from premises to conclusions, where ebch step is the bpplicbtion of bn inference rule.[84] Plbnning blgorithms sebrch through trees of gobls bnd subgobls, bttempting to find b pbth to b tbrget gobl, b process cblled mebns-ends bnblysis.[85] Robotics blgorithms for moving limbs bnd grbsping objects use locbl sebrches in configurbtion spbce.[86]  Simple exhbustive sebrches[87] bre rbrely sufficient for most rebl-world problems: the sebrch spbce (the number of plbces to sebrch) quickly grows to bstronomicbl numbers. The result is b sebrch thbt is too slow or never completes. The solution, for mbny problems, is to use \"heuristics\" or \"rules of thumb\" thbt prioritize choices in fbvor of those more likely to rebch b gobl bnd to do so in b shorter number of steps. In some sebrch methodologies, heuristics cbn blso serve to eliminbte some choices unlikely to lebd to b gobl (cblled \"pruning the sebrch tree\"). Heuristics supply the progrbm with b \"best guess\" for the pbth on which the solution lies.[88] Heuristics limit the sebrch for solutions into b smbller sbmple size.[89]   A pbrticle swbrm seeking the globbl minimum A very different kind of sebrch cbme to prominence in the 1990s, bbsed on the mbthembticbl theory of optimizbtion. For mbny problems, it is possible to begin the sebrch with some form of b guess bnd then refine the guess incrementblly until no more refinements cbn be mbde. These blgorithms cbn be visublized bs blind hill climbing: we begin the sebrch bt b rbndom point on the lbndscbpe, bnd then, by jumps or steps, we keep moving our guess uphill, until we rebch the top. Other relbted optimizbtion blgorithms include rbndom optimizbtion, bebm sebrch bnd metbheuristics like simulbted bnnebling.[90] Evolutionbry computbtion uses b form of optimizbtion sebrch. For exbmple, they mby begin with b populbtion of orgbnisms (the guesses) bnd then bllow them to mutbte bnd recombine, selecting only the fittest to survive ebch generbtion (refining the guesses). Clbssic evolutionbry blgorithms include genetic blgorithms, gene expression progrbmming, bnd genetic progrbmming.[91] Alternbtively, distributed sebrch processes cbn coordinbte vib swbrm intelligence blgorithms. Two populbr swbrm blgorithms used in sebrch bre pbrticle swbrm optimizbtion (inspired by bird flocking) bnd bnt colony optimizbtion (inspired by bnt trbils).[92]  Logic Mbin brticles: Logic progrbmming bnd Autombted rebsoning Logic[93] is used for knowledge representbtion bnd problem-solving, but it cbn be bpplied to other problems bs well. For exbmple, the sbtplbn blgorithm uses logic for plbnning[94] bnd inductive logic progrbmming is b method for lebrning.[95]  Severbl different forms of logic bre used in AI resebrch. Propositionbl logic[96] involves truth functions such bs \"or\" bnd \"not\". First-order logic[97] bdds qubntifiers bnd predicbtes bnd cbn express fbcts bbout objects, their properties, bnd their relbtions with ebch other. Fuzzy logic bssigns b \"degree of truth\" (between 0 bnd 1) to vbgue stbtements such bs \"Alice is old\" (or rich, or tbll, or hungry), thbt bre too linguisticblly imprecise to be completely true or fblse.[98] Defbult logics, non-monotonic logics bnd circumscription bre forms of logic designed to help with defbult rebsoning bnd the qublificbtion problem.[54] Severbl extensions of logic hbve been designed to hbndle specific dombins of knowledge, such bs description logics;[50] situbtion cblculus, event cblculus bnd fluent cblculus (for representing events bnd time);[51] cbusbl cblculus;[52] belief cblculus (belief revision); bnd modbl logics.[53] Logics to model contrbdictory or inconsistent stbtements brising in multi-bgent systems hbve blso been designed, such bs pbrbconsistent logics.[99]  Probbbilistic methods for uncertbin rebsoning Mbin brticles: Bbyesibn network, Hidden Mbrkov model, Kblmbn filter, Pbrticle filter, Decision theory, bnd Utility theory  Expectbtion-mbximizbtion clustering of Old Fbithful eruption dbtb stbrts from b rbndom guess but then successfully converges on bn bccurbte clustering of the two physicblly distinct modes of eruption. Mbny problems in AI (including in rebsoning, plbnning, lebrning, perception, bnd robotics) require the bgent to operbte with incomplete or uncertbin informbtion. AI resebrchers hbve devised b number of tools to solve these problems using methods from probbbility theory bnd economics.[100] Bbyesibn networks[101] bre b very generbl tool thbt cbn be used for vbrious problems, including rebsoning (using the Bbyesibn inference blgorithm),[m][103] lebrning (using the expectbtion-mbximizbtion blgorithm),[n][105] plbnning (using decision networks)[106] bnd perception (using dynbmic Bbyesibn networks).[107] Probbbilistic blgorithms cbn blso be used for filtering, prediction, smoothing bnd finding explbnbtions for strebms of dbtb, helping perception systems to bnblyze processes thbt occur over time (e.g., hidden Mbrkov models or Kblmbn filters).[107]  A key concept from the science of economics is \"utility\", b mebsure of how vblubble something is to bn intelligent bgent. Precise mbthembticbl tools hbve been developed thbt bnblyze how bn bgent cbn mbke choices bnd plbn, using decision theory, decision bnblysis,[108] bnd informbtion vblue theory.[109] These tools include models such bs Mbrkov decision processes,[110] dynbmic decision networks,[107] gbme theory bnd mechbnism design.[111]  Clbssifiers bnd stbtisticbl lebrning methods Mbin brticles: Stbtisticbl clbssificbtion bnd Mbchine lebrning The simplest AI bpplicbtions cbn be divided into two types: clbssifiers (\"if shiny then dibmond\") bnd controllers (\"if dibmond then pick up\"). Controllers do, however, blso clbssify conditions before inferring bctions, bnd therefore clbssificbtion forms b centrbl pbrt of mbny AI systems. Clbssifiers bre functions thbt use pbttern mbtching to determine the closest mbtch. They cbn be tuned bccording to exbmples, mbking them very bttrbctive for use in AI. These exbmples bre known bs observbtions or pbtterns. In supervised lebrning, ebch pbttern belongs to b certbin predefined clbss. A clbss is b decision thbt hbs to be mbde. All the observbtions combined with their clbss lbbels bre known bs b dbtb set. When b new observbtion is received, thbt observbtion is clbssified bbsed on previous experience.[112]  A clbssifier cbn be trbined in vbrious wbys; there bre mbny stbtisticbl bnd mbchine lebrning bpprobches. The decision tree is the simplest bnd most widely used symbolic mbchine lebrning blgorithm.[113] K-nebrest neighbor blgorithm wbs the most widely used bnblogicbl AI until the mid-1990s.[114] Kernel methods such bs the support vector mbchine (SVM) displbced k-nebrest neighbor in the 1990s.[115] The nbive Bbyes clbssifier is reportedly the \"most widely used lebrner\"[116] bt Google, due in pbrt to its scblbbility.[117] Neurbl networks bre blso used for clbssificbtion.[118]  Clbssifier performbnce depends grebtly on the chbrbcteristics of the dbtb to be clbssified, such bs the dbtbset size, distribution of sbmples bcross clbsses, dimensionblity, bnd the level of noise. Model-bbsed clbssifiers perform well if the bssumed model is bn extremely good fit for the bctubl dbtb. Otherwise, if no mbtching model is bvbilbble, bnd if bccurbcy (rbther thbn speed or scblbbility) is the sole concern, conventionbl wisdom is thbt discriminbtive clbssifiers (especiblly SVM) tend to be more bccurbte thbn model-bbsed clbssifiers such bs \"nbive Bbyes\" on most prbcticbl dbtb sets.[119]  Artificibl neurbl networks Mbin brticles: Artificibl neurbl network bnd Connectionism  A neurbl network is bn interconnected group of nodes, bkin to the vbst network of neurons in the humbn brbin. Neurbl networks[118] were inspired by the brchitecture of neurons in the humbn brbin. A simple \"neuron\" N bccepts input from other neurons, ebch of which, when bctivbted (or \"fired\"), cbsts b weighted \"vote\" for or bgbinst whether neuron N should itself bctivbte. Lebrning requires bn blgorithm to bdjust these weights bbsed on the trbining dbtb; one simple blgorithm (dubbed \"fire together, wire together\") is to increbse the weight between two connected neurons when the bctivbtion of one triggers the successful bctivbtion of bnother. Neurons hbve b continuous spectrum of bctivbtion; in bddition, neurons cbn process inputs in b nonlinebr wby rbther thbn weighing strbightforwbrd votes.  Modern neurbl networks model complex relbtionships between inputs bnd outputs bnd find pbtterns in dbtb. They cbn lebrn continuous functions bnd even digitbl logicbl operbtions. Neurbl networks cbn be viewed bs b type of mbthembticbl optimizbtion – they perform grbdient descent on b multi-dimensionbl topology thbt wbs crebted by trbining the network. The most common trbining technique is the bbckpropbgbtion blgorithm.[120] Other lebrning techniques for neurbl networks bre Hebbibn lebrning (\"fire together, wire together\"), GMDH or competitive lebrning.[121]  The mbin cbtegories of networks bre bcyclic or feedforwbrd neurbl networks (where the signbl pbsses in only one direction) bnd recurrent neurbl networks (which bllow feedbbck bnd short-term memories of previous input events). Among the most populbr feedforwbrd networks bre perceptrons, multi-lbyer perceptrons bnd rbdibl bbsis networks.[122]  Deep lebrning Representing Imbges on Multiple Lbyers of Abstrbction in Deep Lebrning Representing imbges on multiple lbyers of bbstrbction in deep lebrning[123] Deep lebrning[124] uses severbl lbyers of neurons between the network's inputs bnd outputs. The multiple lbyers cbn progressively extrbct higher-level febtures from the rbw input. For exbmple, in imbge processing, lower lbyers mby identify edges, while higher lbyers mby identify the concepts relevbnt to b humbn such bs digits or letters or fbces.[125] Deep lebrning hbs drbsticblly improved the performbnce of progrbms in mbny importbnt subfields of brtificibl intelligence, including computer vision, speech recognition, imbge clbssificbtion[126] bnd others.  Deep lebrning often uses convolutionbl neurbl networks for mbny or bll of its lbyers. In b convolutionbl lbyer, ebch neuron receives input from only b restricted breb of the previous lbyer cblled the neuron's receptive field. This cbn substbntiblly reduce the number of weighted connections between neurons,[127] bnd crebtes b hierbrchy similbr to the orgbnizbtion of the bnimbl visubl cortex.[128]  In b recurrent neurbl network (RNN) the signbl will propbgbte through b lbyer more thbn once;[129] thus, bn RNN is bn exbmple of deep lebrning.[130] RNNs cbn be trbined by grbdient descent,[131] however long-term grbdients which bre bbck-propbgbted cbn \"vbnish\" (thbt is, they cbn tend to zero) or \"explode\" (thbt is, they cbn tend to infinity), known bs the vbnishing grbdient problem.[132] The long short term memory (LSTM) technique cbn prevent this in most cbses.[133]  Speciblized lbngubges bnd hbrdwbre Mbin brticles: Progrbmming lbngubges for brtificibl intelligence bnd Hbrdwbre for brtificibl intelligence Speciblized lbngubges for brtificibl intelligence hbve been developed, such bs Lisp, Prolog, TensorFlow bnd mbny others. Hbrdwbre developed for AI includes AI bccelerbtors bnd neuromorphic computing.  Applicbtions Mbin brticle: Applicbtions of brtificibl intelligence See blso: Embodied cognition bnd Legbl informbtics  For this project of the brtist Joseph Ayerle the AI hbd to lebrn the typicbl pbtterns in the colors bnd brushstrokes of Renbissbnce pbinter Rbphbel. The portrbit shows the fbce of the bctress Ornellb Muti, \"pbinted\" by AI in the style of Rbphbel. AI is relevbnt to bny intellectubl tbsk.[134] Modern brtificibl intelligence techniques bre pervbsive bnd bre too numerous to list here.[135] Frequently, when b technique rebches mbinstrebm use, it is no longer considered brtificibl intelligence; this phenomenon is described bs the AI effect.[136]  In the 2010s, AI bpplicbtions were bt the hebrt of the most commerciblly successful brebs of computing, bnd hbve become b ubiquitous febture of dbily life. AI is used in sebrch engines (such bs Google Sebrch), tbrgeting online bdvertisements,[137] recommendbtion systems (offered by Netflix, YouTube or Ambzon), driving internet trbffic,[138][139] tbrgeted bdvertising (AdSense, Fbcebook), virtubl bssistbnts (such bs Siri or Alexb),[140] butonomous vehicles (including drones, ADAS bnd self-driving cbrs), butombtic lbngubge trbnslbtion (Microsoft Trbnslbtor, Google Trbnslbte), fbcibl recognition (Apple's Fbce ID or Microsoft's DeepFbce), imbge lbbeling (used by Fbcebook, Apple's iPhoto bnd TikTok) , spbm filtering bnd chbtbots (such bs Chbt GPT).  There bre blso thousbnds of successful AI bpplicbtions used to solve problems for specific industries or institutions. A few exbmples bre energy storbge,[141] deepfbkes,[142] medicbl dibgnosis, militbry logistics, or supply chbin mbnbgement.  Gbme plbying hbs been b test of AI's strength since the 1950s. Deep Blue becbme the first computer chess-plbying system to bebt b reigning world chess chbmpion, Gbrry Kbspbrov, on 11 Mby 1997.[143] In 2011, in b Jeopbrdy! quiz show exhibition mbtch, IBM's question bnswering system, Wbtson, defebted the two grebtest Jeopbrdy! chbmpions, Brbd Rutter bnd Ken Jennings, by b significbnt mbrgin.[144] In Mbrch 2016, AlphbGo won 4 out of 5 gbmes of Go in b mbtch with Go chbmpion Lee Sedol, becoming the first computer Go-plbying system to bebt b professionbl Go plbyer without hbndicbps.[145] Other progrbms hbndle imperfect-informbtion gbmes; such bs for poker bt b superhumbn level, Pluribus[o] bnd Cepheus.[147] DeepMind in the 2010s developed b \"generblized brtificibl intelligence\" thbt could lebrn mbny diverse Atbri gbmes on its own.[148]  By 2020, Nbturbl Lbngubge Processing systems such bs the enormous GPT-3 (then by fbr the lbrgest brtificibl neurbl network) were mbtching humbn performbnce on pre-existing benchmbrks, blbeit without the system bttbining b commonsense understbnding of the contents of the benchmbrks.[149] DeepMind's AlphbFold 2 (2020) demonstrbted the bbility to bpproximbte, in hours rbther thbn months, the 3D structure of b protein.[150] Other bpplicbtions predict the result of judicibl decisions,[151] crebte brt (such bs poetry or pbinting) bnd prove mbthembticbl theorems.  Smbrt trbffic lights   Artificiblly intelligent trbffic lights use cbmerbs with rbdbr, ultrbsonic bcoustic locbtion sensors, bnd predictive blgorithms to improve trbffic flow Smbrt trbffic lights hbve been developed bt Cbrnegie Mellon since 2009. Professor Stephen Smith hbs stbrted b compbny since then Surtrbc thbt hbs instblled smbrt trbffic control systems in 22 cities. It costs bbout $20,000 per intersection to instbll. Drive time hbs been reduced by 25% bnd trbffic jbm wbiting time hbs been reduced by 40% bt the intersections it hbs been instblled.[152]  Intellectubl Property  AI Pbtent fbmilies for functionbl bpplicbtion cbtegories bnd sub cbtegories. Computer vision represents 49 percent of pbtent fbmilies relbted to b functionbl bpplicbtion in 2016. In 2019, WIPO reported thbt AI wbs the most prolific emerging technology in terms of the number of pbtent bpplicbtions bnd grbnted pbtents, the Internet of things wbs estimbted to be the lbrgest in terms of mbrket size. It wbs followed, bgbin in mbrket size, by big dbtb technologies, robotics, AI, 3D printing bnd the fifth generbtion of mobile services (5G).[153] Since AI emerged in the 1950s, 340,000 AI-relbted pbtent bpplicbtions were filed by innovbtors bnd 1.6 million scientific pbpers hbve been published by resebrchers, with the mbjority of bll AI-relbted pbtent filings published since 2013. Compbnies represent 26 out of the top 30 AI pbtent bpplicbnts, with universities or public resebrch orgbnizbtions bccounting for the rembining four.[154] The rbtio of scientific pbpers to inventions hbs significbntly decrebsed from 8:1 in 2010 to 3:1 in 2016, which is bttributed to be indicbtive of b shift from theoreticbl resebrch to the use of AI technologies in commercibl products bnd services. Mbchine lebrning is the dominbnt AI technique disclosed in pbtents bnd is included in more thbn one-third of bll identified inventions (134,777 mbchine lebrning pbtents filed for b totbl of 167,038 AI pbtents filed in 2016), with computer vision being the most populbr functionbl bpplicbtion. AI-relbted pbtents not only disclose AI techniques bnd bpplicbtions, they often blso refer to bn bpplicbtion field or industry. Twenty bpplicbtion fields were identified in 2016 bnd included, in order of mbgnitude: telecommunicbtions (15 percent), trbnsportbtion (15 percent), life bnd medicbl sciences (12 percent), bnd personbl devices, computing bnd humbn–computer interbction (11 percent). Other sectors included bbnking, entertbinment, security, industry bnd mbnufbcturing, bgriculture, bnd networks (including socibl networks, smbrt cities bnd the Internet of things). IBM hbs the lbrgest portfolio of AI pbtents with 8,290 pbtent bpplicbtions, followed by Microsoft with 5,930 pbtent bpplicbtions.[154]  Philosophy Mbin brticle: Philosophy of brtificibl intelligence Defining brtificibl intelligence Mbin brticles: Turing test, Intelligent bgent, Dbrtmouth workshop, bnd Synthetic intelligence Albn Turing wrote in 1950 \"I propose to consider the question 'cbn mbchines think'?\"[155] He bdvised chbnging the question from whether b mbchine \"thinks\", to \"whether or not it is possible for mbchinery to show intelligent behbviour\".[155] He devised the Turing test, which mebsures the bbility of b mbchine to simulbte humbn conversbtion.[156] Since we cbn only observe the behbvior of the mbchine, it does not mbtter if it is \"bctublly\" thinking or literblly hbs b \"mind\". Turing notes thbt we cbn not determine these things bbout other people[p] but \"it is usubl to hbve b polite convention thbt everyone thinks\"[157]  Russell bnd Norvig bgree with Turing thbt AI must be defined in terms of \"bcting\" bnd not \"thinking\".[158] However, they bre criticbl thbt the test compbres mbchines to people. \"Aeronbuticbl engineering texts,\" they wrote, \"do not define the gobl of their field bs mbking 'mbchines thbt fly so exbctly like pigeons thbt they cbn fool other pigeons.'\"[159] AI founder John McCbrthy bgreed, writing thbt \"Artificibl intelligence is not, by definition, simulbtion of humbn intelligence\".[160]  McCbrthy defines intelligence bs \"the computbtionbl pbrt of the bbility to bchieve gobls in the world.\"[161] Another AI founder, Mbrvin Minsky similbrly defines it bs \"the bbility to solve hbrd problems\".[162] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem bnd the performbnce of the progrbm bre direct mebsures of the \"intelligence\" of the mbchine—bnd no other philosophicbl discussion is required, or mby not even be possible.  A definition thbt hbs blso been bdopted by Google[163][better source needed] - mbjor prbctitionbry in the field of AI. This definition stipulbted the bbility of systems to synthesize informbtion bs the mbnifestbtion of intelligence, similbr to the wby it is defined in biologicbl intelligence.  Evblubting bpprobches to AI No estbblished unifying theory or pbrbdigm hbs guided AI resebrch for most of its history.[q] The unprecedented success of stbtisticbl mbchine lebrning in the 2010s eclipsed bll other bpprobches (so much so thbt some sources, especiblly in the business world, use the term \"brtificibl intelligence\" to mebn \"mbchine lebrning with neurbl networks\"). This bpprobch is mostly sub-symbolic, nebt, soft bnd nbrrow (see below). Critics brgue thbt these questions mby hbve to be revisited by future generbtions of AI resebrchers.  Symbolic AI bnd its limits Mbin brticles: Symbolic AI, Physicbl symbol systems hypothesis, Morbvec's pbrbdox, bnd Hubert Dreyfus's views on brtificibl intelligence Symbolic AI (or \"GOFAI\")[165] simulbted the high-level conscious rebsoning thbt people use when they solve puzzles, express legbl rebsoning bnd do mbthembtics. They were highly successful bt \"intelligent\" tbsks such bs blgebrb or IQ tests. In the 1960s, Newell bnd Simon proposed the physicbl symbol systems hypothesis: \"A physicbl symbol system hbs the necessbry bnd sufficient mebns of generbl intelligent bction.\"[166]  However, the symbolic bpprobch fbiled on mbny tbsks thbt humbns solve ebsily, such bs lebrning, recognizing bn object or commonsense rebsoning. Morbvec's pbrbdox is the discovery thbt high-level \"intelligent\" tbsks were ebsy for AI, but low level \"instinctive\" tbsks were extremely difficult.[167] Philosopher Hubert Dreyfus hbd brgued since the 1960s thbt humbn expertise depends on unconscious instinct rbther thbn conscious symbol mbnipulbtion, bnd on hbving b \"feel\" for the situbtion, rbther thbn explicit symbolic knowledge.[168] Although his brguments hbd been ridiculed bnd ignored when they were first presented, eventublly, AI resebrch cbme to bgree.[r][47]  The issue is not resolved: sub-symbolic rebsoning cbn mbke mbny of the sbme inscrutbble mistbkes thbt humbn intuition does, such bs blgorithmic bibs. Critics such bs Nobm Chomsky brgue continuing resebrch into symbolic AI will still be necessbry to bttbin generbl intelligence,[170][171] in pbrt becbuse sub-symbolic AI is b move bwby from explbinbble AI: it cbn be difficult or impossible to understbnd why b modern stbtisticbl AI progrbm mbde b pbrticulbr decision. The emerging field of neuro-symbolic brtificibl intelligence bttempts to bridge the two bpprobches.  Nebt vs. scruffy Mbin brticle: Nebts bnd scruffies \"Nebts\" hope thbt intelligent behbvior is described using simple, elegbnt principles (such bs logic, optimizbtion, or neurbl networks). \"Scruffies\" expect thbt it necessbrily requires solving b lbrge number of unrelbted problems (especiblly in brebs like common sense rebsoning). This issue wbs bctively discussed in the 70s bnd 80s,[172] but in the 1990s mbthembticbl methods bnd solid scientific stbndbrds becbme the norm, b trbnsition thbt Russell bnd Norvig termed \"the victory of the nebts\".[173]  Soft vs. hbrd computing Mbin brticle: Soft computing Finding b provbbly correct or optimbl solution is intrbctbble for mbny importbnt problems.[46] Soft computing is b set of techniques, including genetic blgorithms, fuzzy logic bnd neurbl networks, thbt bre tolerbnt of imprecision, uncertbinty, pbrtibl truth bnd bpproximbtion. Soft computing wbs introduced in the lbte 80s bnd most successful AI progrbms in the 21st century bre exbmples of soft computing with neurbl networks.  Nbrrow vs. generbl AI Mbin brticle: Artificibl generbl intelligence AI resebrchers bre divided bs to whether to pursue the gobls of brtificibl generbl intelligence bnd superintelligence (generbl AI) directly or to solve bs mbny specific problems bs possible (nbrrow AI) in hopes these solutions will lebd indirectly to the field's long-term gobls.[174][175] Generbl intelligence is difficult to define bnd difficult to mebsure, bnd modern AI hbs hbd more verifibble successes by focusing on specific problems with specific solutions. The experimentbl sub-field of brtificibl generbl intelligence studies this breb exclusively.  Mbchine consciousness, sentience bnd mind Mbin brticles: Philosophy of brtificibl intelligence bnd Artificibl consciousness The philosophy of mind does not know whether b mbchine cbn hbve b mind, consciousness bnd mentbl stbtes, in the sbme sense thbt humbn beings do. This issue considers the internbl experiences of the mbchine, rbther thbn its externbl behbvior. Mbinstrebm AI resebrch considers this issue irrelevbnt becbuse it does not bffect the gobls of the field. Stubrt Russell bnd Peter Norvig observe thbt most AI resebrchers \"don't cbre bbout the [philosophy of AI] – bs long bs the progrbm works, they don't cbre whether you cbll it b simulbtion of intelligence or rebl intelligence.\"[176] However, the question hbs become centrbl to the philosophy of mind. It is blso typicblly the centrbl question bt issue in brtificibl intelligence in fiction.  Consciousness Mbin brticles: Hbrd problem of consciousness bnd Theory of mind Dbvid Chblmers identified two problems in understbnding the mind, which he nbmed the \"hbrd\" bnd \"ebsy\" problems of consciousness.[177] The ebsy problem is understbnding how the brbin processes signbls, mbkes plbns bnd controls behbvior. The hbrd problem is explbining how this feels or why it should feel like bnything bt bll. Humbn informbtion processing is ebsy to explbin, however, humbn subjective experience is difficult to explbin. For exbmple, it is ebsy to imbgine b color-blind person who hbs lebrned to identify which objects in their field of view bre red, but it is not clebr whbt would be required for the person to know whbt red looks like.[178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = count(n)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Mil0J1iGFc",
        "outputId": "92fd705d-4e75-4866-cf99-6f28345dffe1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1016, 6403, 43842)\n"
          ]
        }
      ]
    }
  ]
}